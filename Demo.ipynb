{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network from Kell, Yamins, Shook, Norman-Haignere, McDermott, 2018\n",
    "\n",
    "This notebook shows how to create a tensorflow graph for the network with the weights and biases used in <a href=\"https://www.cell.com/neuron/fulltext/S0896-6273(18)30250-2\">Kell et al., 2018</a>. This notebook also gives an example of how to pass a sound into the network.\n",
    "\n",
    "\n",
    "### Note on network input\n",
    "\n",
    "The input to the network is a \"cochleagram\", a time-frequency decomposition of a sound that is similar to a spectrogram. Below we provide examples of how to pass a pre-computed cochleagram into the network, as well as how to compute the cochleagram for an example wav and then pass that cochleagram to the network.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "Most of the dependencies to run this are relatively standard. However, please note the following:\n",
    "- This notebook was tested and run with version 1.5.0 of `tensorflow`. It was not tested with other versions.\n",
    "- `pycochleagram` is a module to generate cochleagrams to pass sounds into the network, which can be found <a href=\"https://github.com/mcdermottLab/pycochleagram\">here</a>.\n",
    "- `PIL` is the Python Image Library.\n",
    "\n",
    "### Contact\n",
    "If you have any questions, please contact Alex Kell. Email: < first_name >< last_name >@mit.edu.\n",
    "\n",
    "Thanks, and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "import sys\n",
    "sys.path.append('./network/')\n",
    "from network.branched_network_class import branched_network\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib as plt \n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the following to run demo_from_wav()\n",
    "from pycochleagram import cochleagram as cgram \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some helper functions\n",
    "def resample(example, new_size):\n",
    "    im = Image.fromarray(example)\n",
    "    resized_image = im.resize(new_size, resample=Image.ANTIALIAS)\n",
    "    return np.array(resized_image)\n",
    "\n",
    "def plot_cochleagram(cochleagram, title): \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.matshow(cochleagram.reshape(256,256), origin='lower',cmap=plt.cm.Blues, fignum=False, aspect='auto')\n",
    "    plt.yticks([]); plt.xticks([]); plt.title(title); \n",
    "    \n",
    "def play_wav(wav_f, sr, title):   \n",
    "    print (title+':')\n",
    "    ipd.display(ipd.Audio(wav_f, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_pre_generated_cochleagram():\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    net_object = branched_network() # make network object\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy') #Load logits to word key \n",
    "    music_key = np.load('./demo_stim/logits_to_genre_key.npy') #Load logits to genre key\n",
    "\n",
    "    # example pre-generated speech cochleagram \n",
    "    example_cochleagram = np.load('./demo_stim/example_cochleagram_0.npy') \n",
    "    plot_cochleagram(example_cochleagram,'Example speech cochleagram' )\n",
    "\n",
    "    # run cochleagram through network and get logits for word branch\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: example_cochleagram})\n",
    "\n",
    "    # determine word branch prediction \n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print (\"Speech Example ... actual label: according  predicted_label: \" + prediction[0] +'\\n')\n",
    "    \n",
    "    # example pre-generated music cochleagram\n",
    "    example_cochleagram_music = np.load('./demo_stim/example_cochleagram_1.npy') \n",
    "    plot_cochleagram(example_cochleagram_music,'Example music cochleagram' )\n",
    "    \n",
    "    # run cochleagram through network and get logits for genre branch\n",
    "    logits_music = net_object.session.run(net_object.genre_logits, \n",
    "                                          feed_dict={net_object.x: example_cochleagram_music})\n",
    "    # note: throughout paper top-5 accuracy is reported for genre task\n",
    "    prediction_music = (logits_music).argsort()[:,-5:][0][::-1] \n",
    "    print (\"Music Example... actual label: \"+ music_key[11]+ \"  top-5 predicted_labels (in order of confidence): \")\n",
    "    print (\"\\n\"+ \"; \".join(music_key[prediction_music]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cochleagram(wav_f, sr, title):\n",
    "    # define parameters\n",
    "    n, sampling_rate = 50, 16000\n",
    "    low_lim, hi_lim = 20, 8000\n",
    "    sample_factor, pad_factor, downsample = 4, 2, 200\n",
    "    nonlinearity, fft_mode, ret_mode = 'power', 'auto', 'envs'\n",
    "    strict = True\n",
    "\n",
    "    # create cochleagram\n",
    "    c_gram = cgram.cochleagram(wav_f, sr, n, low_lim, hi_lim, \n",
    "                               sample_factor, pad_factor, downsample,\n",
    "                               nonlinearity, fft_mode, ret_mode, strict)\n",
    "    \n",
    "    # rescale to [0,255]\n",
    "    c_gram_rescaled =  255*(1-((np.max(c_gram)-c_gram)/np.ptp(c_gram)))\n",
    "    \n",
    "    # reshape to (256,256)\n",
    "    c_gram_reshape_1 = np.reshape(c_gram_rescaled, (211,400))\n",
    "    c_gram_reshape_2 = resample(c_gram_reshape_1,(256,256))\n",
    "    \n",
    "    plot_cochleagram(c_gram_reshape_2, title)\n",
    "\n",
    "    # prepare to run through network -- i.e., flatten it\n",
    "    c_gram_flatten = np.reshape(c_gram_reshape_2, (1, 256*256)) \n",
    "    \n",
    "    return c_gram_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_from_wav():\n",
    "    tf.reset_default_graph()\n",
    "    net_object = branched_network()\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy') # load logits to word key\n",
    "    music_key = np.load('./demo_stim/logits_to_genre_key.npy') # load logits to word key \n",
    "    \n",
    "    \n",
    "    # generate cochleagram, then pass cochleagram through network and get logits for word branch\n",
    "    \n",
    "    ## Speech examples\n",
    "    \n",
    "    # example 1:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_1.wav') # note the sampling rate is 16000hz.\n",
    "    play_wav(wav_f, sr, 'Example 1')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 1')\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print (\"Speech Example ... \\n clean speech, actual label: Increasingly, predicted_label: \" \\\n",
    "        + prediction[0] +'\\n')\n",
    "    \n",
    "    ## Music examples \n",
    "    \n",
    "    # example 6:\n",
    "    sr, wav_f = wav.read('./demo_stim/example_6.wav') \n",
    "    play_wav(wav_f, sr, 'Example 6')\n",
    "    c_gram = generate_cochleagram(wav_f, sr, 'Example 6')\n",
    "    logits = net_object.session.run(net_object.genre_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = (logits).argsort()[:,-5:][0][::-1] \n",
    "    print (\"Music Example ... \\n Background: Auditory Scene, snr: -3db, actual label: \" \\\n",
    "        + music_key[1] + \",\\n top-5 predicted_labels (in order of confidence): \\n \" \\\n",
    "        + \";\\n \".join(music_key[prediction]) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reshape() missing 1 required positional argument: 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4a60c198a195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo_pre_generated_cochleagram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-0ad601ef79c2>\u001b[0m in \u001b[0;36mdemo_pre_generated_cochleagram\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnet_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbranched_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make network object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./demo_stim/logits_to_word_key.npy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Load logits to word key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmusic_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./demo_stim/logits_to_genre_key.npy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Load logits to genre key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Antonin/Code/speech_recog/network/branched_network_class.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#Make the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenre_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Antonin/Code/speech_recog/network/branched_network_class.py\u001b[0m in \u001b[0;36mget_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m#Shared layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         x_reshape = tf.reshape(shape=[None, self.layer_params_dict['data']['edge']*self.layer_params_dict['data']['edge']], [-1, self.layer_params_dict['data']['edge'], \n\u001b[0m\u001b[1;32m     96\u001b[0m                                         self.layer_params_dict['data']['edge'],1])\n\u001b[1;32m     97\u001b[0m         \u001b[0mh_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_params_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_vars_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reshape() missing 1 required positional argument: 'shape'"
     ]
    }
   ],
   "source": [
    "demo_pre_generated_cochleagram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "demo_from_wav()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
